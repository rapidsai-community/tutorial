{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BinKvOgMYOCp"
      },
      "source": [
        "# cuML's accelerator mode (cuml.accel)\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/rapidsai-community/tutorial/refs/heads/main/images/cuml-accel-exec-flow.png\" style=\"float: right; margin-left: 5px; width: 300px;\">\n",
        "\n",
        "cuML is a Python GPU library for accelerating machine learning models using a scikit-learn-like API.\n",
        "\n",
        "cuML now has an accelerator mode (cuml.accel) which allows you to bring accelerated computing to existing workflows\n",
        "with zero code changes required. \n",
        "\n",
        "Support for:\n",
        "\n",
        "- scikit-learn\n",
        "- umap-learn (UMAP)\n",
        "- hdbscan (HDBSCAN)\n",
        "\n",
        "\n",
        "Estimators that are implemented in cuML will be dispatched to run on the GPU where possible, and fall back to the CPU \n",
        "library otherwise. \n",
        "\n",
        "If a model is constructed on the GPU and then a method is called that is not implemented in `cuML`, `cuml.accel` will \n",
        "reconstruct the model on the CPU and gracefully fall back to the equivalent scikit-learn function instead.\n",
        "\n",
        "\n",
        "**Attribution:** This section of the tutorial is based on the `cuML.accel` [quickstart notebook](https://colab.research.google.com/github/rapidsai-community/showcase/blob/main/getting_started_tutorials/cuml_sklearn_colab_demo.ipynb) from the RAPIDS documentation.\n",
        "\n",
        "This notebook is a brief introduction to `cuml.accel`. With classical machine learning, there is a wide range of interesting \n",
        "problems we can explore. In this tutorial we'll examine three of the more popular use cases: classification, clustering, \n",
        "and dimensionality reduction.\n",
        "\n",
        "### Data \n",
        "\n",
        "If you are running this locally, and you followed the steps in notebook [0.Welcome_and_Setup.ipynb](https://github.com/rapidsai-community/tutorial/blob/main/0.Welcome_and_Setup.ipynb), \n",
        "you should have the `/data` folder ready to go. \n",
        "\n",
        "#### Google Colab Instructions\n",
        "\n",
        "In the next step we download a script that will allow you to get the data for this notebook session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# colab: uncomment next line to get the data setup script\n",
        "#! wget https://raw.githubusercontent.com/rapidsai-community/tutorial/refs/heads/main/data_setup.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# colab: uncomment next line to get the pageviews data set\n",
        "#! python data_setup.py --cover-type --har"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKWtiMvJAS60",
        "outputId": "3ad02328-d81d-4296-f2f9-9c97baf40114"
      },
      "outputs": [],
      "source": [
        "# Verify that you are running with an NVIDIA GPU\n",
        "! nvidia-smi  # this should display information about available GPUs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M37-8qsDa2Pe"
      },
      "source": [
        "# Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vT5RNLwdce-O"
      },
      "source": [
        "Let's load a dataset and see how we can use scikit-learn to classify that data.  For this example we'll use the Coverage Type dataset, \n",
        "which contains a number of features that can be used to predict forest cover type, such as elevation, aspect, slope, and soil-type.\n",
        "\n",
        "More information on this dataset can be found at [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/31/covertype)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PKt2Lje5lYQw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rLHSPlLnv-1y"
      },
      "outputs": [],
      "source": [
        "columns = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n",
        "           'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
        "           'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3',\n",
        "           'Wilderness_Area4', 'Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5', 'Soil_Type6',\n",
        "           'Soil_Type7', 'Soil_Type8', 'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12', 'Soil_Type13',\n",
        "           'Soil_Type14', 'Soil_Type15', 'Soil_Type16', 'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20',\n",
        "           'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26', 'Soil_Type27',\n",
        "           'Soil_Type28', 'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34',\n",
        "           'Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40', 'Cover_Type']\n",
        "\n",
        "data = pd.read_csv(\"data/cover_forest_type.csv\", names=columns, header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53P_F5oHmh9F",
        "outputId": "da81ae3e-9725-4a99-d11e-e65f57a99d73"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext cuml.accel\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Mz-yThWmlqg"
      },
      "source": [
        "We separate out the classification variable (`Cover_Type`) from the rest of the data. This is what we will aim to predict \n",
        "with our classification model. We can also split our dataset into training and test data using the scikit-learn train_test_split function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "heVSKrDLxdN3"
      },
      "outputs": [],
      "source": [
        "X, y = data.drop(\"Cover_Type\", axis=1), data[\"Cover_Type\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        },
        "id": "8v0QxJXrViiR",
        "outputId": "5510c2ad-5ea7-4cb7-c3b0-0527d836fc6b"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "clf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=5,\n",
        "    max_features=1.0,\n",
        "    n_jobs=-1,\n",
        ")\n",
        "clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqJC2jtmqpva"
      },
      "source": [
        "For reference, this same model trained on CPU it takes more than two minutes.  \n",
        "\n",
        "Using cuML we're able to train this random forest model in just seconds instead of minutes. One thing to note is that \n",
        "cuML's implementation of `RandomForestClassifier` doesn't utilize the `n_jobs` parameter like scikit-learn, but we still\n",
        "accept it which makes it easier to use this accelerator with zero code changes.\n",
        "\n",
        "Let's take a look at an accuracy score and classification report. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVre6kav6iaS",
        "outputId": "2c240c5f-b7c2-4eac-98b5-aee48bd4dcc7"
      },
      "outputs": [],
      "source": [
        "y_pred = clf.predict(X_test)\n",
        "cr = classification_report(y_test, y_pred)\n",
        "print(cr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmcwmqN4q4cv"
      },
      "source": [
        "With a model that runs in just seconds, we can perform hyperparameter optimization using a method like the grid search, and \n",
        "have results in just minutes instead of hours.\n",
        "\n",
        "You can try something like this, (it'll take > 10 min):\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid to search over\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['sqrt', 'log2'],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "Having one model trained in such short time allow us to quickly iterate on the hyperparameter configuration and find a \n",
        "model that performs better with excellent speedups.\n",
        "\n",
        "For example, let's see what happens with a different `max_depth`\n",
        "\n",
        "**Exercise:** Train the `RandomForestClassifier` with a  different set of values and analyze the results. \n",
        "\n",
        "<details>\n",
        "  <summary>Solution (click dropdown) </summary>\n",
        "  <p>\n",
        "\n",
        "```python\n",
        "# to run this type it in a code cell\n",
        "clf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=30,\n",
        "    max_features=1.0,\n",
        "    n_jobs=-1,\n",
        ")\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "```\n",
        "  </p>\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your solution here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdjvwlZ-DaoW"
      },
      "source": [
        "# Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70sdcTIdfzkK"
      },
      "source": [
        "Clustering is an important data science workflow because it helps uncover hidden patterns and structures within data without requiring labeled outcomes. In practice, with high dimensional data it can be difficult to discern whether the clusters we've chosen are good or not. One way to determine the quality of our clustering is with sklearn's [silhouette score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score), which we'll examine shortly.\n",
        "\n",
        "HDBSCAN is a popular density-based clustering algorithm that is highly flexible. We'll load a toy sklearn dataset to illustrate how HDBSCAN can be accelerated with cuml.accel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "oIlRPHZ1DZ7q"
      },
      "outputs": [],
      "source": [
        "import hdbscan\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.metrics import silhouette_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "RqXkNWRjDZ9n",
        "outputId": "57435e6c-a303-4531-d64b-87ba14cce410"
      },
      "outputs": [],
      "source": [
        "N = 20000\n",
        "K = 100\n",
        "\n",
        "X, y = make_blobs(\n",
        "    n_samples=N,\n",
        "    n_features=K,\n",
        "    centers=5,\n",
        "    cluster_std=[3, 1, 2, 1.5, 0.5],\n",
        "    random_state=42,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "id": "536jWwBWg1Ou",
        "outputId": "c589e021-e14e-4848-d8b5-2302c00d4ef7"
      },
      "outputs": [],
      "source": [
        "clusterer = hdbscan.HDBSCAN()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "clusterer.fit(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrAzGgqrxHtY",
        "outputId": "05493c84-d6cf-469a-e533-db36558c76ec"
      },
      "outputs": [],
      "source": [
        "print(silhouette_score(X, clusterer.labels_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91a7zkUIoE5d"
      },
      "source": [
        "It's important to note that on real-world datasets, the silhouette score produced by the GPU and CPU implementations of HDBSCAN will often have slight differences. The cuML implementation of HDBSCAN should provide equivalent results, but it is normal for the actual clusters to vary slightly when dealing with complex datasets.\n",
        "\n",
        "Lastly, let's take a look at how we can use cuml's accelerator mode for a third popular machine learning task -- dimensionality reduction. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93qD18LqDiOj"
      },
      "source": [
        "# Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0mwvzC0rdq4"
      },
      "source": [
        "UMAP is a popular dimensionality reduction technique that is used for both data visualization and as preprocessing for downstream modeling due to its ability to balance preserving both local and global structure of high-dimensional data. To learn more about how it works, visit the [UMAP documentation](https://umap-learn.readthedocs.io/en/latest/).\n",
        "\n",
        "To explore how cuML can accelerate UMAP, let's load in another dataset from UCI. We'll use the Human Activity Recognition (HAR) dataset, which was created from recordings of 30 subjects performing activities of daily living (ADL) while carrying a waist-mounted smartphone with embedded inertial sensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "QjGw0WDwCu2_"
      },
      "outputs": [],
      "source": [
        "X_train = pd.read_csv(\n",
        "    \"data/UCI_HAR_Dataset/train/X_train.txt\", sep=r\"\\s+\", header=None\n",
        ")\n",
        "y_train = pd.read_csv(\n",
        "    \"data/UCI_HAR_Dataset/train/y_train.txt\", sep=r\"\\s+\", header=None\n",
        ")\n",
        "X_test = pd.read_csv(\n",
        "    \"data/UCI_HAR_Dataset/test/X_test.txt\", sep=r\"\\s+\", header=None\n",
        ")\n",
        "y_test = pd.read_csv(\n",
        "    \"data/UCI_HAR_Dataset/test/y_test.txt\", sep=r\"\\s+\", header=None\n",
        ")\n",
        "labels = pd.read_csv(\n",
        "    \"data/UCI_HAR_Dataset/activity_labels.txt\", sep=r\"\\s+\", header=None\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oesYc97pQ1vr",
        "outputId": "e7dca260-5abb-4f27-d61e-e336955b5271"
      },
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5Cle6mGOH7S"
      },
      "source": [
        "Let's take a look at the activity labels to better understand the data we're working with. We can see that the sensors have grouped activities into 6 different classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "22YYMRvXOUUz",
        "outputId": "b15af6d7-8c52-474e-f102-f573dd7f2d2e"
      },
      "outputs": [],
      "source": [
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "8rA7SsEpCu5M"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Scale the data before applying UMAP\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9ltJhG7tQt0"
      },
      "source": [
        "Let's run UMAP with some basic parameters and explore a lower-dimensionality projection of this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4sR7x3sF_9a"
      },
      "outputs": [],
      "source": [
        "import umap\n",
        "umap_model = umap.UMAP(\n",
        "    n_neighbors=15, n_components=2, random_state=42, min_dist=0.0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGD49PFVlyrz",
        "outputId": "81f5a19c-d2c6-498c-c673-facc3ced964d"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# Fit UMAP model to the data\n",
        "X_train_umap = umap_model.fit_transform(X_train_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwJC2woSl4U9"
      },
      "source": [
        "It's often quite interesting to visualize the resulting projection of the embeddings created by UMAP. In this case, let's take a look at the now 2-dimensional dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "id": "PDRxaBEdF__X",
        "outputId": "2ddd2b8b-8b1a-48e4-e06b-b5ffe0245f27"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the UMAP result\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(\n",
        "    X_train_umap[:, 0],\n",
        "    X_train_umap[:, 1],\n",
        "    c=y_train.values.ravel(),\n",
        "    cmap=\"Spectral\",\n",
        "    s=10,\n",
        ")\n",
        "plt.colorbar(label=\"Activity\")\n",
        "plt.title(\"UMAP projection of the UCI HAR dataset\")\n",
        "plt.xlabel(\"UMAP Component 1\")\n",
        "plt.ylabel(\"UMAP Component 2\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKFBLuE7mFrs"
      },
      "source": [
        "It's interesting to see how our different categories are grouped in relation to one another.\n",
        "\n",
        "We can look at the trustworthiness score to better understand how well the structure of the original dataset was preserved by our 2D projection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Say1EIdqMLoq",
        "outputId": "3c3f1623-8ef2-4703-9dcb-1e2ba3e148d7"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import trustworthiness\n",
        "trustworthiness(X_train, X_train_umap, n_neighbors=15)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEAEGvqHztTI"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we learned how `cuml.accel` works with familiar libraries by providing GPU acceleration with zero code changes:\n",
        "\n",
        "- Fast data normalization with `StandardScaler`\n",
        "- Efficient `UMAP` dimensionality reduction.\n",
        "- Efficient clustering with HDBSCAN\n",
        "- High data trustworthiness score while reducing compute time.\n",
        "\n",
        "\n",
        "For more information on getting started with `cuml.accel`, check out [RAPIDS.ai](https://rapids.ai/cuml-accel/) or the [cuML Docs](https://docs.rapids.ai/api/cuml/stable/).\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "rapids-tutorial",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
