{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluating Machine Learning Models with cuML\n",
    "\n",
    "cuML is NVIDIA's GPU-accelerated machine learning library that implements popular ML algorithms with CUDA optimization. It \n",
    "provides scikit-learn-like APIs while leveraging GPU acceleration to deliver significant speedups compared to CPU-based \n",
    "implementations. cuML is part of the RAPIDS suite of open-source software libraries.\n",
    "\n",
    "This notebook explores several basic machine learning estimators in cuML, demonstrating how to train them and evaluate them \n",
    "with built-in metrics functions. All of the models are trained on synthetic data, generated by cuML's dataset utilities.\n",
    "\n",
    "1. Random Forest Classifier\n",
    "2. UMAP\n",
    "3. DBSCAN\n",
    "4. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cuml\n",
    "from cupy import asnumpy \n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classification and Accuracy metrics\n",
    "\n",
    "The Random Forest classification algorithm builds several decision trees, and aggregates each of their outputs to \n",
    "make a prediction. For more information on cuML's implementation of the Random Forest Classification model please refer \n",
    "to: [Random Forest Classifier API Documentation](https://docs.rapids.ai/api/cuml/stable/api.html#cuml.ensemble.RandomForestClassifier)\n",
    "\n",
    "Accuracy score is the ratio of correct predictions to the total number of predictions. It is used to measure the performance \n",
    "of classification models. For more information on the accuracy score metric please refer to [Wikipedia's article on Accuracy and Precision](https://en.wikipedia.org/wiki/Accuracy_and_precision).\n",
    "\n",
    "For more information on cuML's implementation of accuracy score metrics please refer to the [cuML Accuracy Score API Documentation](https://docs.rapids.ai/api/cuml/stable/api.html#cuml.metrics.accuracy.accuracy_score).\n",
    "\n",
    "The cell below shows an end to end pipeline of the Random Forest Classification model. Here the dataset was generated by \n",
    "using scikit-learn's make_classification dataset. The generated dataset was used to train and run predict on the model. \n",
    "Random forest's performance is evaluated and then compared between the values obtained from the cuML and scikit-learn accuracy metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.datasets.classification import make_classification\n",
    "from cuml.model_selection import train_test_split\n",
    "from cuml.ensemble import RandomForestClassifier as cuRF\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# synthetic dataset dimensions\n",
    "n_samples = 1000\n",
    "n_features = 10\n",
    "n_classes = 2\n",
    "\n",
    "# random forest depth and size\n",
    "n_estimators = 25\n",
    "max_depth = 10\n",
    "\n",
    "# generate synthetic data [ binary classification task ]\n",
    "X, y = make_classification(\n",
    "    n_classes=n_classes,\n",
    "    n_features=n_features,\n",
    "    n_samples=n_samples,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "model = cuRF(\n",
    "    max_depth=max_depth,\n",
    "    n_estimators=n_estimators,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "trained_RF = model.fit(X_train, y_train)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "cu_score = cuml.metrics.accuracy_score(y_test, predictions)\n",
    "sk_score = accuracy_score(asnumpy(y_test), asnumpy(predictions))\n",
    "\n",
    "print(\" cuml accuracy: \", cu_score)\n",
    "print(\" sklearn accuracy : \", sk_score)\n",
    "\n",
    "# save\n",
    "dump(trained_RF, \"RF.model\")\n",
    "\n",
    "# to reload the model uncomment the line below\n",
    "# loaded_model = load('RF.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP and Trustworthiness metrics\n",
    "UMAP is a dimensionality reduction algorithm that performs non-linear dimension reduction. It can also be used for visualization.\n",
    "For additional information on the UMAP model please refer to the [RAPIDS UMAP documentation](https://docs.rapids.ai/api/cuml/stable/api.html#cuml.UMAP)\n",
    "\n",
    "Trustworthiness is a measure of the extent to which the local structure is retained in the embedding of the model. Therefore, \n",
    "if a sample predicted by the model lies within the unexpected region of the nearest neighbors, then those samples would be penalized. \n",
    "\n",
    "**Additional Resources:**\n",
    "- [scikit-learn trustworthiness documentation](https://scikit-learn.org/dev/modules/generated/sklearn.manifold.t_sne.trustworthiness.html)\n",
    "- [cuML trustworthiness documentation](https://docs.rapids.ai/api/cuml/stable/api.html#cuml.metrics.trustworthiness.trustworthiness)\n",
    "\n",
    "The cell below shows an end to end pipeline of UMAP model. The blobs dataset is created by cuML's equivalent of `make_blobs` \n",
    "function to be used as the input. The output of UMAP's fit_transform is evaluated using the trustworthiness function. The\n",
    "values obtained by scikit-learn and cuML's trustworthiness are compared below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.datasets import make_blobs\n",
    "from cuml.manifold.umap import UMAP as cuUMAP\n",
    "from sklearn.manifold import trustworthiness\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n_samples = 1000\n",
    "n_features = 100\n",
    "cluster_std = 0.1\n",
    "\n",
    "X_blobs, y_blobs = make_blobs(\n",
    "    n_samples=n_samples,\n",
    "    cluster_std=cluster_std,\n",
    "    n_features=n_features,\n",
    "    random_state=0,\n",
    "    dtype=np.float32,\n",
    ")\n",
    "\n",
    "trained_UMAP = cuUMAP(n_neighbors=10).fit(X_blobs)\n",
    "X_embedded = trained_UMAP.transform(X_blobs)\n",
    "\n",
    "cu_score = cuml.metrics.trustworthiness(X_blobs, X_embedded)\n",
    "sk_score = trustworthiness(asnumpy(X_blobs), asnumpy(X_embedded))\n",
    "\n",
    "print(\"cuml's trustworthiness score:\", cu_score)\n",
    "print(\"sklearn's trustworthiness score:\", sk_score)\n",
    "\n",
    "# save\n",
    "dump(trained_UMAP, \"UMAP.model\")\n",
    "\n",
    "# to reload the model uncomment the line below\n",
    "# loaded_model = load('UMAP.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN and Adjusted Random Index\n",
    "\n",
    "DBSCAN is a popular and powerful clustering algorithm. For additional information on the DBSCAN model please refer to the \n",
    "[DBSCAN documentation](https://docs.rapids.ai/api/cuml/stable/api.html#cuml.DBSCAN)\n",
    "\n",
    "We create the blobs dataset using the cuML equivalent of `make_blobs` function.\n",
    "\n",
    "Adjusted random index is a metric which is used to measure the similarity between two data clusters, and it is adjusted to \n",
    "take into consideration the chance grouping of elements.\n",
    "For more information on Adjusted random index please refer to: [Wikipedia Rand index documentation](https://en.wikipedia.org/wiki/Rand_index)\n",
    "\n",
    "The cell below shows an end to end model of DBSCAN. The output of DBSCAN's fit_predict is evaluated using the Adjusted \n",
    "Random Index function. The values obtained by scikit-learn and cuML's adjusted random metric are compared below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml import DBSCAN as cumlDBSCAN\n",
    "from sklearn.metrics import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "n_features = 100\n",
    "cluster_std = 0.1\n",
    "\n",
    "X_blobs, y_blobs = make_blobs(\n",
    "    n_samples=n_samples,\n",
    "    n_features=n_features,\n",
    "    cluster_std=cluster_std,\n",
    "    random_state=0,\n",
    "    dtype=np.float32,\n",
    ")\n",
    "\n",
    "cuml_dbscan = cumlDBSCAN(eps=3, min_samples=2)\n",
    "\n",
    "trained_DBSCAN = cuml_dbscan.fit(X_blobs)\n",
    "\n",
    "cu_y_pred = trained_DBSCAN.fit_predict(X_blobs)\n",
    "\n",
    "cu_adjusted_rand_index = cuml.metrics.cluster.adjusted_rand_score(y_blobs, cu_y_pred)\n",
    "sk_adjusted_rand_index = adjusted_rand_score(asnumpy(y_blobs), asnumpy(cu_y_pred))\n",
    "\n",
    "print(f\"cuml's adjusted random index score: {cu_adjusted_rand_index}\")\n",
    "print(f\"sklearn's adjusted random index score: {sk_adjusted_rand_index}\")\n",
    "\n",
    "# save and optionally reload\n",
    "dump(trained_DBSCAN, \"DBSCAN.model\")\n",
    "\n",
    "# to reload the model uncomment the line below\n",
    "# loaded_model = load('DBSCAN.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression and  R^2 score\n",
    "Linear Regression is a simple machine learning model where the response `y` is modeled by a linear combination of the \n",
    "predictors in `X`.\n",
    "\n",
    "`R^2` score is also known as the coefficient of determination. It is used as a metric for scoring regression models. It\n",
    "scores the output of the model based on the proportion of total variation of the model. For more information on the `R^2`\n",
    "score metrics please refer to [Wikipedia page on Coefficient of determination](https://en.wikipedia.org/wiki/Coefficient_of_determination)\n",
    "\n",
    "For more information on cuML's implementation of the `r2` score metrics please refer to [cuML R2 Score Documentation](https://docs.rapids.ai/api/cuml/stable/api.html)\n",
    "\n",
    "The cell below uses the Linear Regression model to compare the results between cuML and scikit-learn trustworthiness metric. For more \n",
    "information on cuML's implementation of the Linear Regression model please refer to [cuML Linear Regression Documentation](https://docs.rapids.ai/api/cuml/stable/api.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.datasets import make_regression\n",
    "from cuml.model_selection import train_test_split\n",
    "from cuml.linear_model import LinearRegression as cuLR\n",
    "from sklearn.metrics import r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "n_samples = 2**10\n",
    "n_features = 100\n",
    "n_info = 70\n",
    "\n",
    "X_reg, y_reg = make_regression(\n",
    "    n_samples=n_samples,\n",
    "    n_features=n_features,\n",
    "    n_informative=n_info,\n",
    "    random_state=123,\n",
    ")\n",
    "\n",
    "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(\n",
    "    X_reg, y_reg, train_size=0.8, random_state=10\n",
    ")\n",
    "\n",
    "cuml_reg_model = cuLR(fit_intercept=True, normalize=True, algorithm=\"eig\")\n",
    "\n",
    "trained_LR = cuml_reg_model.fit(X_reg_train, y_reg_train)\n",
    "cu_preds = trained_LR.predict(X_reg_test)\n",
    "\n",
    "cu_r2 = cuml.metrics.r2_score(y_reg_test, cu_preds)\n",
    "sk_r2 = r2_score(asnumpy(y_reg_test), asnumpy(cu_preds))\n",
    "\n",
    "print(f\"cuml's r2 score : {cu_r2}\")\n",
    "print(f\"sklearn's r2 score : {sk_r2}\")\n",
    "\n",
    "# save and reload\n",
    "dump(trained_LR, \"LR.model\")\n",
    "\n",
    "# to reload the model uncomment the line below \n",
    "# loaded_model = load('LR.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example integration with an existing workflow\n",
    "Chances are that you already have an existing workflow that uses `scikit-learn`. You may even have custom transformers implemented for data preprocessing.\n",
    "\n",
    "This example takes a pipeline that uses [skrub](https://skrub-data.org/stable/index.html), an open-source package that aims at bridging the gap between tabular data sources and machine-learning models.\n",
    "\n",
    "Here we will show how different tools that leverage the `scikit-learn` API can be combined in a pipeline.\n",
    "## Easy learning on a dataframe\n",
    "\n",
    "Let's first retrieve the dataset, using one of the downloaders from the `skrub.datasets` module. As all the downloaders,\n",
    "`~skrub.datasets.fetch_employee_salaries` returns a dataset with attributes `X`, and `y`. `X` is a dataframe which contains \n",
    "the features (aka design matrix, explanatory variables, independent variables). `y` is a column (pandas Series) which\n",
    "contains the target (aka dependent, response variable) that we want to learn to predict from `X`. In this case `y` is the annual salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub.datasets import fetch_employee_salaries\n",
    "\n",
    "dataset = fetch_employee_salaries()\n",
    "employees, salaries = dataset.X, dataset.y\n",
    "employees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most machine-learning algorithms work with arrays of numbers. The challenge here is that the employees dataframe is a \n",
    "heterogeneous set of columns: some are numerical (`'year_first_hired'`), some dates (`'date_first_hired'`), some have a few \n",
    "categorical entries (`'gender'`), some many (`'employee_position_title'`). Therefore our table needs to be \"vectorized\": processed\n",
    "to extract numeric features.\n",
    "\n",
    "`skrub` provides a custom transformer, called a `TableVectorizer` to preprocess the data for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub import TableVectorizer\n",
    "\n",
    "vectorizer = TableVectorizer()\n",
    "vectorized_employees = vectorizer.fit_transform(employees)\n",
    "vectorized_employees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple Pipeline for tabular data\n",
    "\n",
    "The `TableVectorizer` outputs data that can be understood by a scikit-learn estimator. Therefore we can easily build a\n",
    "2-step scikit-learn `Pipeline` that we can fit, test or cross-validate and that works well on tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipeline = make_pipeline(TableVectorizer(), RandomForestRegressor())\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# In colab this will take ~ 3-4min to run - you might want to skip this cell\n",
    "results = cross_validate(pipeline, employees, salaries)\n",
    "scores = results[\"test_score\"]\n",
    "print(f\"R2 score:  mean: {np.mean(scores):.3f}; std: {np.std(scores):.3f}\")\n",
    "print(f\"mean fit time: {np.mean(results['fit_time']):.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's swap out `scikit-learn`'s `RandomForestRegressor` for `cuML`'s and notice that it runs in seconds instead of \n",
    "minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.ensemble import RandomForestRegressor as cuRFR\n",
    "\n",
    "pipeline = make_pipeline(TableVectorizer(), cuRFR())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "results = cross_validate(pipeline, employees, salaries)\n",
    "scores = results[\"test_score\"]\n",
    "print(f\"R2 score:  mean: {np.mean(scores):.3f}; std: {np.std(scores):.3f}\")\n",
    "print(f\"mean fit time: {np.mean(results['fit_time']):.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we learned:\n",
    "\n",
    "* How to use cuML's drop-in replacements for scikit-learn estimators\n",
    "* How cuML can accelerate machine learning workflows on GPU\n",
    "* How to integrate cuML with scikit-learn Pipelines\n",
    "* The performance benefits of GPU-accelerated machine learning with cuML\n",
    "\n",
    "To learn more, we encourage you to visit the [cuML documentation](https://docs.rapids.ai/api/cuml/stable/)\n",
    "\n",
    "In the next notebook we will learn about `cuml.accel`:\n",
    "\n",
    "[Next Notebook: 5 cuml.accel â†’](https://colab.research.google.com/github/rapidsai-community/tutorial/blob/main/5.cuml_accel.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
